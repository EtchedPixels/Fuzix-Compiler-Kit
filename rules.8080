# Branches
	jmp %1
%1:
=
%1:

	jmp %2
%1:
%2:
=
%1:
%2:

	jmp %1
	jmp %2
=
	jmp %1

# Trivial
	xchg
	xchg
=

	xchg
;
	xchg
=

	push h
	pop h
=

	push h
;
	pop h
=

	xchg
	pop d
	xchg
=
	pop h

# Compare 0 cast from char
	mvi h,0
	call __cmpeq0
=
	call __cmpeq0b

# Compare 0 cast from char
	mvi h,0
	call __cmpne0
=
	call __cmpne0b

# Assign 0L
	push h
	lxi h,0
	shld __hireg
	call __assignl
=
	call __assign0l

# Remove do {} while (0);
%1:
	lxi h,0
	call __bool
;
	jnz %1
=
%1:
	lxi h,0

# Optimize repeated call pattern (the ; matters here)
#	pop d
#;
#	mvi l,%1
#	push h
#=
#	mvi l,%1
#	xthl
#
#	pop d
#;
#	lxi h,%1
#	push h
#=
#	lxi h,%1
#	xthl

# comparisons with a cast when not to zero
	mvi h,0
	lxi d,%1
	call __cmp%2
=
	mvi e,<%1
	call __cmp%2b

# the same pattern occurs a lot for small multiplication
	mvi h,0
	lxi d,%1
	call __mulde%2
=
	lxi d,%1
	call __mulde%2b

# we may rework this if we build args as we go, but in the mean time
	push h
	lxi h,%1
	pop psw
	push h
=
	lxi h,%1
	push h

	push h
	lhld %1
	pop psw
	push h
=
	lhld %1
	push h

# And via xthl
	push h
	lxi h,%1
	xthl
=
	lxi h,%1
	push h

	push h
	lhld %1
	xthl
=
	lhld %1
	push h

# Common postinc/dec
	push h
	lxi h,1
	call __postinc
=
	call __postinc1

	push h
	lxi h,2
	call __postinc
=
	call __postinc2

	push h
	lxi h,1
	call __postdec
=
	call __postdec1

	push h
	lxi h,2
	call __postdec
=
	call __postdec2

# Common postinc/dec via d
	push d
	lxi h,1
	call __postinc
=
	call __postinc1d

	push d
	lxi h,2
	call __postinc
=
	call __postinc2d

	push d
	lxi h,1
	call __postdec
=
	call __postdec1d

	push d
	lxi h,2
	call __postdec
=
	call __postdec2d

# Common pre inc/dec
	lxi d,1
	call __pluseqde
=
	call __pluseq1

	lxi d,2
	call __pluseqde
=
	call __pluseq2

	lxi d,1
	call __minuseqde
=
	call __minuseq1

	lxi d,2
	call __minuseqde
=
	call __minuseq2

# Common pre inc/dec via d
	push d
	lxi h,1
	call __pluseq
=
	call __pluseq1d

	push d
	lxi h,2
	call __pluseq
=
	call __pluseq2d

	push d
	lxi h,1
	call __minuseq
=
	call __minuseq1d

	push d
	lxi h,2
	call __minuseq
=
	call __minuseq2d

	xchg
	call __pluseq1
=
	call __pluseq1d

	xchg
	call __pluseq2
=
	call __pluseq2d

	xchg
	call __minuseq1
=
	call __minuseq1d

	xchg
	call __minuseq2
=
	call __minuseq2d

# Common pattern in C	 if (x & c) 
	call __bandde
	call __bool
=
	call __bbandde

# Merge array/struct offsetting (fixme - should be constify cleaned)
	lxi d,%1
	dad d
	lxi d,%2
	dad d
=
	lxi d,%1+%2
	dad d

# for loop cleanup
L%1_l:
;
	jmp L%1_n
L%1_c:
;
	jmp L%1_l
L%1_n:
=
L%1_l:
L%1_c:
L%1_n:

# Remove always false/true paths
	lxi h,1
	call __bool
;
	jz %1
=
;

	lxi h,0
	call __bool
	jz %1
=
	jmp %1

	lxi h,1
	call __bool
;
	jnz %1
=
	jmp %1

	lxi h,0
	call __bool
;
	jnz %1
=
;

# Tidy up some load patterns
	push h
	lxi h,%1
	xchg
	pop h
=
	lxi d,%1

	mov l,a
	call boolc
=
	call boolc_a

# Register tidy
#	mov l,c
#	mov h,b
#	push h
#;
#=
#	push b
#;

# Silly stuff
	inx h
	dcx h
=

	dcx h
	inx h
=

# derefl is commonly linked to an add hl,sp
	add hl,sp
	call __derefl
=
	call __dereflsp

# Useful trick - a load does not disturb flags
# Don't do it for memory loads as they might be side effects
# until we add some better magic for volatile
# Func exit variant
	jz %1
	lxi h,%2
;
	ret
%1:
	lxi h,%3
;
	ret
=
	lxi h,%3
	rz
	lxi h,%2
%1:
	ret

	ret
%1:
	ret
=
%1:
	ret

# Byte deref for condition shorteners (the ; check means the accumulator
# is no longer live
#	mov l,m
#	mov a,h
#	ora l
#;
#=
#	mov a,m
#	ora a
#
#	mov l,m
#	mov a,l
#	ora a
#;
#=
#	mov a,m
#	ora a
#
#
# Fake up inline functions for some cases. Speeds up some I/O stuff a fair
# bit without going into asm or adding weird __sfr stuff
#
	lxi h,%1
	push h
	call ___builtin_in
	pop psw
=
	in <%1
	mov l,a

	lxi h,%1
	push h
	lxi h,%2
	push h
	call __builtin_out
	pop psw
	pop psw
=
	mvi a,<%1
	out <%2

	push h
	lxi h,%1
	push h
	call __builtin_out
	pop psw
	pop psw
=
	mov a,l
	out <%1

#	Reloads between A and L
	mov l,a
	mov a,l
=
	mov l,a
